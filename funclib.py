# -*- coding: utf-8 -*-
"""funclib

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KbpO4RykP5kQL6uDAUXktqXwvjK1OHip
"""

import numpy as np
import pandas as pd
import numba as nb


@nb.jit(nopython = True)
def np_shift(arr, num, fill_value=np.nan):
  '''
  Shift numpy array 'aar' with 'num' as pandas shift
  '''
  result = np.empty_like(arr)
  if num > 0:
      result[:num] = fill_value
      result[num:] = arr[:-num]
  elif num < 0:
      result[num:] = fill_value
      result[:num] = arr[-num:]
  else:
      result[:] = arr
  return result


@nb.jit(nopython = True)
def ratio(x, y):
  '''
  Fast jitted ration between elevent of 2 equal sized arrays
  '''
  return x/y


def trail_stop3(df, n):
  '''
  Function to get max drawdown in the next 'n' observations computing 
  with columns 'high' & 'low' of df. It uses numba so it is quick.
  Parameters
  ----------
  df : pd.DataFrame
      The price dataframe.
  n : integer
      Days looking forward in the series
  Returns
  -------
  pd.Series
      Series contaiting max drawdow
  '''
  import warnings
  warnings.filterwarnings(action='ignore', message='All-NaN slice encountered')

  low = df.low.values
  high = df.high.values
  tuples = [(ih, il) for ih in np.arange(0,n) for il in np.arange(ih,n)][1:]
  loss ={}
  for ih, il in tuples:
    loss[f'{ih}_{il}'] = ratio(np_shift(low, -il), np_shift(high, -ih))
  loss = np.column_stack( list(loss.values()) )

  return pd.Series(np.nanmin(loss, axis=1)-1, index=df.index)


def trail_stop2(df, n):
  '''
  Function to get max drawdown in the next 'n' observations computing 
  with columns 'high' & 'low' of df. It uses pandas series.
  Parameters
  ----------
  df : pd.DataFrame
      The price dataframe.
  n : integer
      Days looking forward in the series
  Returns
  -------
  pd.Series
      Series contaiting max drawdow
  '''
  tuples = [(ih, il) for ih in np.arange(0,n) for il in np.arange(ih,n)][1:]
  loss = {}
  for ih, il in tuples:
    loss[f'{ih}_{il}'] = df.low.shift(-il) / df.high.shift(-ih)
  loss = pd.concat(loss, axis=1)

  return round(loss.min(axis=1)-1,4)


def pivots(cd):
    cd=cd.copy()
    ser = cd.high
    cd['peaks'] = ser[(ser.shift(1) <= ser) & (ser.shift(-1) <= ser)]
    cd['peaks'].fillna(method='ffill', inplace=True)
    ser = cd.low
    cd['dips'] = ser[(ser.shift(1) >= ser) & (ser.shift(-1) >= ser)]
    cd['dips'].fillna(method='ffill', inplace=True)
    peaks=cd['peaks']
    dips=cd['dips']
    mids = (peaks + dips) / 2
    return peaks, dips, mids


def pivots_interpolated(cd):
  cd=cd.copy()
  ser = cd.high
  cd['peaks'] = ser[(ser.shift(1) <= ser) & (ser.shift(-1) <= ser)]
  cd['peaks'] = cd.peaks.interpolate()
  ser = cd.low
  cd['dips'] = ser[(ser.shift(1) >= ser) & (ser.shift(-1) >= ser)]
  cd['dips'] = cd.dips.interpolate()
  peaks=cd['peaks']
  dips=cd['dips']
  mids = (peaks + dips) / 2
  return peaks, dips, mids


def combine_historical_ohlc(first, second):
  """
  Combine 2 MultiIndex containing ohlc data.
  NaN values in 'first' are filled with non-NaNs from 'second' on the same index.
  Checks wheather for a given ticker there were stock splits and updates old
  values from 'first' with new values after split.

  Parameters
  ----------
  first : pandas.core.frame.DataFrame
      older MultiIndex dataframe
  second : pandas.core.frame.DataFrame
      newer MultiIndex dataframe

  Returns
  -------
  pandas.core.frame.DataFrame
  """
  ano = []
  #iterate through all tickers in second
  for i in list(second.columns.levels[0]):
    #if a ticker has all NaN's it means it is delisted, so we leave it aside
    if second[i].close.isna().all():
      continue
    # if a ticker is in both second and first we want to check if it has splits
    if i in list(first.columns.levels[0]):
      #get the overlapping indexes
      intersect = (second[i].dropna().index).intersection(first[i].dropna().index)
      #check for inequalities where the second and first overlap
      x = (second[i].loc[intersect, 'close']).ne(first[i].loc[intersect, 'close'])
      if x.sum() > 0 :
        #store the list of tickers that exibit inequalities
        print(f'{x.sum()} Inequalities at ticker: {i}')
        ano.append(i)

  #combine first and second
  ohlc_total = first.combine_first(second)
  #download the full date range for the tickers that might have suffered splits
  if ano != []:
    ano_df = pd.DataFrame(data=ano, columns=['Ticker'])
    fresh = get_ohlc(ano_df, start_date=first.index.min(), end_date=second.index.max())
    ohlc_total.update(fresh)

  return ohlc_total



######################## Time warp ##################

@nb.jit(nopython = True)
def get_cost_matrix(ts1: np.array, ts2: np.array) -> np.array:
    '''
    Get the dynamic time warping cost matrix, which is used to determine
    the warping path and hence the overall cost of the path.
    
    Parameters
    ----------
    ts1 : np.array
        The first time series to compare.
    ts2 : np.array
        The second time series to compare.
    
    Returns
    -------
    C : np.array
        The dynamic time warping cost matrix.
    '''
    
    # Initialise a full cost matrix, filled with np.inf. This is so we can
    # start the algorithm and not get stuck on the boundary
    C = np.full(shape = (ts1.shape[0] + 1, ts2.shape[0] + 1), 
               fill_value = np.inf,)
    
    # Place the corner to zero, so that we don't have the minimum of 3 infs
    C[0, 0] = 0
    
    for i in range(1, ts1.shape[0] + 1):
        for j in range(1, ts2.shape[0] + 1):
            
            # Euclidian distance between the two points
            dist = abs(ts1[i-1] - ts2[j-1])
            
            # Find the cheapest cost of all three neighbours
            prev_min = min(C[i-1, j], C[i, j-1], C[i-1, j-1])
            
            # Populate the entry in the cost matrix
            C[i, j] = dist + prev_min
            
    return C[1:, 1:]

@nb.jit(nopython = True)
def get_path_cost(C: np.array):
    '''
    Get the optimal path and overall cost of the path.
    
    Parameters
    ----------
    C : np.array
        The DTW cost matrix.
    
    Returns
    -------
    path, cost : Tuple[list, float]
        The optimal path coordinates and the overall cost.
    '''
    
    i = C.shape[0] - 1
    j = C.shape[1] - 1
    
    path = [[i, j]]

    while (i > 0) | (j > 0):
        
        min_cost = min(C[i-1, j-1], C[i-1, j], C[i, j-1])
        
        if min_cost == C[i-1, j-1]:
            i -= 1
            j -= 1
        elif min_cost == C[i-1, j]:
            i -= 1
        elif min_cost == C[i, j-1]:
            j -= 1
        
        path.append([i, j])
        
    return path, C[-1, -1]


@nb.jit(nopython = True)
def get_avg_cost(ts: np.array, breakouts: list) -> float:
    '''
    Compare the time series with all the breakout examples, and return the
    mean of all costs.
    Parameters
    ----------
    ts : np.array
        The time series we are comparing.
    breakouts : list
        A list of time series with the breakout examples.
    Returns
    -------
    float
        The mean of top 3 best costs from the time series comparisons.
    '''
    
    costs = []
    
    for i in range(len(breakouts)):
        C = get_cost_matrix(ts, breakouts[i])
        _, path_cost = get_path_cost(C.astype(np.float64))
        
        costs.append(path_cost)
        avg_cost = np.mean(np.sort(np.array(costs))[:3])
            
    return avg_cost


@nb.jit(nopython = True)
def standard_scale(ts: np.array) -> np.array:
    return (ts - np.mean(ts))/np.std(ts)


def get_time_series(df: pd.DataFrame,
                    date_start: str,
                    date_end: str,
                    attribute: str='close') -> np.array:
    '''
    Filter the price dataframe to the specified range, and scale using a z
    score scaling approach.
    Parameters
    ----------
    df : pd.DataFrame
        The price dataframe.
    date_start : str
        Starting date for the time series in the format yyyy-mm-dd
    date_end : str
        Ending date for the time series in the format yyyy-mm-dd
    attribute : str
        Column in df; default is the closing price 'close', else to be specified
        among other series, e.g: 'open', 'RSI', 'EMA50'
    Returns
    -------
    np.array
        The scaled time series
    '''
    
    df = df[ (df.index >= date_start) & (df.index <= date_end)]
    
    return standard_scale(df[attribute].values)


def load_pattern_examples(examples, ta, attribute='close') -> list:
  '''
  Load all breakout examples for the time-series comparisons
  Parameters
  ----------
  examples: list
    A list of pattern examples where each element is a 3 sized list: 
    ['Ticker', 'start', 'end']
  ta: pd.MultiIndex
    Dataframe containing the 'Ticker' in examples as level0 columns and 'attribute'
    in level1 columns
  attribute: str
    Name of the series whose pattern is under analysis.

  Returns
  -------
  list
      A list of scaled time series for comparisons to be inputed in cost_matrix
  '''
  
  scaled_series = []
  for b in examples:
    
      df = ta[b[0]].copy()
      scaled_series.append(get_time_series(df, b[1], b[2], attribute))
  
  return scaled_series


@nb.jit(nopython = True)
def pattern_scanner(series: np.array, 
                    norm_series: list,
                    length: int,
                    threshold: float) -> np.array:
    '''
    Run the scanner over the entire of the stock history, and return an array
    to indicate whether the region is a breakout candidate (1) or not (0)
    Parameters
    ----------
    series : np.array
        The series to compare (e.g. close price).
    norm_series : list
        A list of time series with the breakout examples.
    length : int
        The lookback period for the scanner.
    threshold : float
        The scanner threshold (values less than this are considered a breakout) 
    Returns
    -------
    np.array
        A binary array indicating the positions where the scanner returns a
        positive result.
    '''
  
    candidates = []
    for idx in range(length, series.shape[0]):
        
        ts = standard_scale(series[idx-length:idx])
        # if ts has NaN do not compute cost matrix; NaN are result of division by 0 (stdev is 0)
        if np.isnan(sum(ts)) == True:
          cost = np.nan
        else:
          cost = get_avg_cost(ts, norm_series)

        if cost < threshold:
            candidates.append(1)
        else:
            candidates.append(0)


    return np.array(candidates)